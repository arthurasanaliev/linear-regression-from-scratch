{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6301c71",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression with One Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0dc35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_input():\n",
    "    n = int(input(\"Enter number of points: \"))\n",
    "    x = list(map(int, input(\"Enter X: \").split()))\n",
    "    y = list(map(int, input(\"Enter Y: \").split()))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def plot_line(x, y, b, flg=0):    \n",
    "    plt.scatter(x, y, color='black')\n",
    "    x_range = range(min(x) - 1, max(x) + 2)\n",
    "    color = 'red'\n",
    "    if flg == 1:\n",
    "        color = 'orange'\n",
    "    elif flg == 2:\n",
    "        color = 'blue'\n",
    "    plt.plot(x_range, [b[1] * x_i + b[0] for x_i in x_range], color=color, label='Regression Line')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    title = 'Gradient Descent'\n",
    "    if flg == 1:\n",
    "        title = 'Stochastic ' + title\n",
    "    elif flg == 2:\n",
    "        title = 'Mini Batch ' + title\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss_function(x, y, vector, losses, b_values, flg=0):\n",
    "    # Plot the loss function (losses) vs iterations\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(losses)), losses)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Mean Squared Error (Loss)')\n",
    "    s = ' '\n",
    "    if flg == 1:\n",
    "        s = ' Stochastic '\n",
    "    elif flg == 2:\n",
    "        s = ' Mini Batch '\n",
    "    title = 'Loss Function During' + s + 'Gradient Descent'\n",
    "    plt.title(title)\n",
    "\n",
    "    # Plot b values vs iterations\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(b_values[0])), b_values[0], label='b0')\n",
    "    plt.plot(range(len(b_values[1])), b_values[1], label='b1')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('b values')\n",
    "    title = 'b values During' + s + 'Gradient Descent'\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# uses MSE/2 as loss function\n",
    "def gradient(x, y, b):\n",
    "    res = b[0] + b[1] * x - y   \n",
    "    \n",
    "    return res.mean(), (res * x).mean()\n",
    "\n",
    "def mse(x, y, b):\n",
    "    return np.mean(np.square(b[0] + b[1] * x - y)) / 2\n",
    "\n",
    "def gradient_descent(gradient, x, y, start, learn_rate=0.01, n_iter=10_000, tolerance=1e-06, dtype=\"float64\"):\n",
    "    dtype_ = np.dtype(dtype)\n",
    "    \n",
    "    n_iter = int(n_iter)\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    \n",
    "    losses = []  # List to store MSE values at each iteration\n",
    "    b_values = [[], []]  # List to store b0 and b1 values at each iteration\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        diff = -learn_rate * np.array(gradient(x, y, vector), dtype_)\n",
    "        losses.append(mse(x, y, vector))\n",
    "        b_values[0].append(vector[0])  # Store b0 value\n",
    "        b_values[1].append(vector[1])  # Store b1 value\n",
    "        \n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "            \n",
    "        vector += diff\n",
    "        \n",
    "    return vector if vector.shape else vector.item(), losses, b_values\n",
    "\n",
    "def stochastic_gradient_descent(gradient, x, y, start, learn_rate=0.01, n_iter=10_000, tolerance=1e-06, dtype=\"float64\", random_state=None):\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    n_iter = int(n_iter)\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "\n",
    "    n_obs = x.shape[0]\n",
    "\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    losses = []\n",
    "    b_values = [[], []]\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        index = rng.choice(n_obs)\n",
    "        x_batch, y_batch = x[index:index+1], y[index:index+1] \n",
    "\n",
    "        grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "        diff = -learn_rate * grad\n",
    "\n",
    "        losses.append(mse(x_batch, y_batch, vector))\n",
    "        b_values[0].append(vector[0])\n",
    "        b_values[1].append(vector[1])\n",
    "\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "\n",
    "        vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item(), losses, b_values\n",
    "\n",
    "def mini_batch_gradient_descent(gradient, x, y, start, learn_rate=0.01, batch_size=32, n_iter=10_000, tolerance=1e-06, dtype=\"float64\", random_state=None):\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    batch_size = int(batch_size)\n",
    "    n_iter = int(n_iter)\n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "\n",
    "    n_obs = x.shape[0]\n",
    "    n_batches = int(np.ceil(n_obs / batch_size))  # Calculate number of batches\n",
    "\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    losses = []  # List to store MSE values at each iteration\n",
    "    b_values = [[], []]  # List to store b0 and b1 values at each iteration\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        rng.shuffle(np.c_[x, y])  # Shuffle data for each iteration (optional)\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            # Define batch indices\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_obs)  # Handle potential last batch\n",
    "\n",
    "            x_batch, y_batch = x[start_idx:end_idx], y[start_idx:end_idx]\n",
    "\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = -learn_rate * grad\n",
    "\n",
    "            losses.append(mse(x_batch, y_batch, vector))  # Calculate loss on mini-batch\n",
    "\n",
    "            b_values[0].append(vector[0])  # Store b0 value\n",
    "            b_values[1].append(vector[1])  # Store b1 value\n",
    "\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                return vector, losses, b_values  # Early stopping with losses and b_values\n",
    "\n",
    "            vector += diff\n",
    "\n",
    "    return vector if vector.shape else vector.item(), losses, b_values\n",
    "\n",
    "def main():\n",
    "    x, y = get_input()\n",
    "    \n",
    "    # Gradient Descent\n",
    "    vector, losses, b_values = gradient_descent(gradient, x, y, start=[0.5, 0.5])\n",
    "    \n",
    "    print()\n",
    "    print(\"Gradient Descent Algorithm\")\n",
    "    print(f\"Intercept: {vector[0]}, slope: {vector[1]}\")\n",
    "\n",
    "    plot_loss_function(x, y, vector, losses, b_values, 0)\n",
    "    plot_line(x, y, vector, 0)\n",
    "    \n",
    "    # Stochastic Gradient Descent\n",
    "    vector_sgd, losses_sgd, b_values_sgd = stochastic_gradient_descent(gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008, n_iter=10_000, random_state=0)\n",
    "    \n",
    "    print()\n",
    "    print(\"Stochastic Gradient Descent Algorithm\")\n",
    "    print(f\"Intercept: {vector_sgd[0]}, slope: {vector_sgd[1]}\")\n",
    "    \n",
    "    plot_loss_function(x, y, vector_sgd, losses_sgd, b_values_sgd, 1)\n",
    "    plot_line(x, y, vector_sgd, 1)\n",
    "    \n",
    "    # Mini Batch Gradient Descent\n",
    "    vector_mbgd, losses_mbgd, b_values_mbgd = mini_batch_gradient_descent(gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008, batch_size=3, n_iter=10_000, random_state=0)\n",
    "    \n",
    "    print()\n",
    "    print(\"Mini Batch Gradient Descent Algorithm\")\n",
    "    print(f\"Intercept: {vector_mbgd[0]}, slope: {vector_mbgd[1]}\")\n",
    "    \n",
    "    plot_loss_function(x, y, vector_mbgd, losses_mbgd, b_values_mbgd, 2)\n",
    "    plot_line(x, y, vector_mbgd, 2)\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
